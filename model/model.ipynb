{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFS--8GVSg5X"
      },
      "source": [
        "Cleaning the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NDQQnjgEggfy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWdGOyfPgxhi"
      },
      "outputs": [],
      "source": [
        "# 1. Load and Preprocess Data (Modified for Couplet Dataset)\n",
        "df = pd.read_csv(\"Ghazal_ur.csv\")  # Update path\n",
        "\n",
        "# Handle missing values and combine couplets\n",
        "df = df.dropna(subset=['misra1', 'misra2'])  # Remove rows with missing lines\n",
        "\n",
        "# Create a continuous text corpus from couplets\n",
        "text_entries = []\n",
        "for _, row in df.iterrows():\n",
        "    text_entries.extend([row['misra1'], row['misra2']])\n",
        "urdu_text = '\\n'.join(text_entries)  # Each line is either misra1 or misra2\n",
        "\n",
        "# Build vocabulary from the combined text\n",
        "vocab = sorted(set(urdu_text))\n",
        "char_to_index = {u: i for i, u in enumerate(vocab)}\n",
        "index_to_char = np.array(vocab)\n",
        "text_as_int = np.array([char_to_index[c] for c in urdu_text])\n",
        "\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(100, drop_remainder=True)\n",
        "dataset = sequences.map(lambda chunk: (chunk[:-1], chunk[1:])).batch(64, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xmQ-OCMxg8VK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import metrics\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss, metrics=[metrics.SparseCategoricalAccuracy()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D_Mj-Ni8hN7y"
      },
      "outputs": [],
      "source": [
        "# Set up checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHRPSehRhRra",
        "outputId": "b673be9c-cc41-441d-9c74-bcf780871918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - loss: 3.2018 - sparse_categorical_accuracy: 0.2652\n",
            "Epoch 2/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - loss: 2.2610 - sparse_categorical_accuracy: 0.3670\n",
            "Epoch 3/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - loss: 2.0720 - sparse_categorical_accuracy: 0.4066\n",
            "Epoch 4/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - loss: 1.9402 - sparse_categorical_accuracy: 0.4365\n",
            "Epoch 5/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - loss: 1.8494 - sparse_categorical_accuracy: 0.4594\n",
            "Epoch 6/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - loss: 1.7802 - sparse_categorical_accuracy: 0.4768\n",
            "Epoch 7/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - loss: 1.7222 - sparse_categorical_accuracy: 0.4915\n",
            "Epoch 8/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 1.6689 - sparse_categorical_accuracy: 0.5052\n",
            "Epoch 9/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 1.6186 - sparse_categorical_accuracy: 0.5187\n",
            "Epoch 10/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 1.5684 - sparse_categorical_accuracy: 0.5322\n",
            "Epoch 11/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 1.5160 - sparse_categorical_accuracy: 0.5471\n",
            "Epoch 12/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - loss: 1.4593 - sparse_categorical_accuracy: 0.5631\n",
            "Epoch 13/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 1.3972 - sparse_categorical_accuracy: 0.5811\n",
            "Epoch 14/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 1.3355 - sparse_categorical_accuracy: 0.5991\n",
            "Epoch 15/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 1.2765 - sparse_categorical_accuracy: 0.6165\n",
            "Epoch 16/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 1.1999 - sparse_categorical_accuracy: 0.6394\n",
            "Epoch 17/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 1.1325 - sparse_categorical_accuracy: 0.6594\n",
            "Epoch 18/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 1.0886 - sparse_categorical_accuracy: 0.6713\n",
            "Epoch 19/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 1.0684 - sparse_categorical_accuracy: 0.6756\n",
            "Epoch 20/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 1.0521 - sparse_categorical_accuracy: 0.6773\n",
            "Epoch 21/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 1.0195 - sparse_categorical_accuracy: 0.6869\n",
            "Epoch 22/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 0.9962 - sparse_categorical_accuracy: 0.6928\n",
            "Epoch 23/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 0.9794 - sparse_categorical_accuracy: 0.6960\n",
            "Epoch 24/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 0.9684 - sparse_categorical_accuracy: 0.6987\n",
            "Epoch 25/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - loss: 0.9534 - sparse_categorical_accuracy: 0.7028\n",
            "Epoch 26/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 0.9390 - sparse_categorical_accuracy: 0.7066\n",
            "Epoch 27/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 0.9288 - sparse_categorical_accuracy: 0.7088\n",
            "Epoch 28/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - loss: 0.9205 - sparse_categorical_accuracy: 0.7109\n",
            "Epoch 29/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 0.9106 - sparse_categorical_accuracy: 0.7138\n",
            "Epoch 30/30\n",
            "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 0.8991 - sparse_categorical_accuracy: 0.7165\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "# 4. Text Generation (Same with Urdu Script Support)\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "model_2.build(tf.TensorShape([1, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmAlDAMshUb5",
        "outputId": "27ae8df1-1192-4209-e7b9-aa3f28e1a6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ckpt_1.weights.h5', 'ckpt_2.weights.h5', 'ckpt_3.weights.h5', 'ckpt_4.weights.h5', 'ckpt_5.weights.h5', 'ckpt_6.weights.h5', 'ckpt_7.weights.h5', 'ckpt_8.weights.h5', 'ckpt_9.weights.h5', 'ckpt_10.weights.h5', 'ckpt_11.weights.h5', 'ckpt_12.weights.h5', 'ckpt_13.weights.h5', 'ckpt_14.weights.h5', 'ckpt_15.weights.h5', 'ckpt_16.weights.h5', 'ckpt_17.weights.h5', 'ckpt_18.weights.h5', 'ckpt_19.weights.h5', 'ckpt_20.weights.h5', 'ckpt_21.weights.h5', 'ckpt_22.weights.h5', 'ckpt_23.weights.h5', 'ckpt_24.weights.h5', 'ckpt_25.weights.h5', 'ckpt_26.weights.h5', 'ckpt_27.weights.h5', 'ckpt_28.weights.h5', 'ckpt_29.weights.h5', 'ckpt_30.weights.h5']\n",
            "Loaded weights from: ./training_checkpoints/ckpt_30.weights.h5\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".weights.h5\")]\n",
        "\n",
        "# Sort by epoch number (numeric part of filename)\n",
        "checkpoint_files.sort(key=lambda f: int(re.search(r'ckpt_(\\d+)', f).group(1)))\n",
        "\n",
        "print(checkpoint_files)\n",
        "if checkpoint_files:\n",
        "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
        "    model_2.load_weights(latest_checkpoint)\n",
        "    print(f\"Loaded weights from: {latest_checkpoint}\")\n",
        "else:\n",
        "    print(\"No checkpoint found!\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULg0ZqKLizqJ",
        "outputId": "96c7ffbf-90f0-4ae7-b346-9f1f65983f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“œ Generated Poetry:\n",
            "\n",
            "Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª Ù„Ø¨ÙˆÚº Ù¾Û Ù†Û Ù„Ø§Ù†Ø§ Ú©Ø¨Ú¾ÛŒ ØªÙ…Ø§Ù… Ø´Ø¨\n",
            "Ø¯Ù„ Ø®ÙˆØ§Ø¨ Ø¬Ùˆ Ù¾Ù†Ø³Ø§Úº Ø¯Ø± Ø¨Ø§Ø± Ø¯ÙˆØ³Øª \n",
            "Ø¯ÙˆØ± Ø³Ø¨Ø² Ø¯Ø´Øª Ø¹Ø´Ù‚ ØªÙˆ Ø¯ÛŒÚ©Ú¾ÙˆÚº Ú¯Ø§ Ø¬Ø³Û’ Ø¢Û ØªÙˆ Ù…Ø± Ú†Ú¾Ø§Ø¦Û’ ÛÙˆØ¦Û’\n",
            "Ø§Ø­Ø¨Ø§Ø¨ Ù¾Ø§ Ø³ÙØ± Ø¯Ø± Ú©ÛŒØ§ ÛÙ…Ø§Ø±ÛŒØ§Úº\n",
            "Ø¯Ø´Øª Ù…ÛŒÚº Ø¬Ø§\n"
          ]
        }
      ],
      "source": [
        "# Generate poetry with Urdu script seed\n",
        "num_generate = 130\n",
        "seed_text = \"Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª Ù„Ø¨ÙˆÚº Ù¾Û Ù†Û Ù„Ø§Ù†Ø§ Ú©Ø¨Ú¾ÛŒ\"  #\n",
        "\n",
        "# Clean seed text using dataset vocabulary\n",
        "seed_text = ''.join([c if c in char_to_index else ' ' for c in seed_text])\n",
        "input_eval = [char_to_index[c] for c in seed_text]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "model_2.layers[1].reset_states()\n",
        "\n",
        "text_generated = [seed_text]\n",
        "for _ in range(num_generate):\n",
        "    predictions = model_2.predict(input_eval, verbose=0)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predicted_id = np.argmax(predictions[-1])\n",
        "    text_generated.append(index_to_char[predicted_id])\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "generated_poetry = ''.join(text_generated)\n",
        "print(\"\\nğŸ“œ Generated Poetry:\\n\")\n",
        "print(generated_poetry)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
